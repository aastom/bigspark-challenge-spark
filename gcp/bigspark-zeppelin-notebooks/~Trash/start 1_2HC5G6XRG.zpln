{
  "paragraphs": [
    {
      "text": "val path \u003d \"/usr/local/tpcds_data/household_demographics.dat\"\nval df \u003d spark.read.csv(path)\ndf.show()",
      "user": "anonymous",
      "dateUpdated": "2022-08-24 16:59:40.969",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------------+\n|               _c0|\n+------------------+\n|    1|2|0-500|0|0||\n|    2|3|0-500|0|0||\n|    3|4|0-500|0|0||\n|    4|5|0-500|0|0||\n|    5|6|0-500|0|0||\n|    6|7|0-500|0|0||\n|    7|8|0-500|0|0||\n|    8|9|0-500|0|0||\n|   9|10|0-500|0|0||\n|  10|11|0-500|0|0||\n|  11|12|0-500|0|0||\n|  12|13|0-500|0|0||\n|  13|14|0-500|0|0||\n|  14|15|0-500|0|0||\n|  15|16|0-500|0|0||\n|  16|17|0-500|0|0||\n|  17|18|0-500|0|0||\n|  18|19|0-500|0|0||\n|  19|20|0-500|0|0||\n|20|1|501-1000|0|0||\n+------------------+\nonly showing top 20 rows\n\n\u001b[1m\u001b[34mpath\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d /usr/local/tpcds_data/household_demographics.dat\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [_c0: string]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://2d4da64570e5:4040/jobs/job?id\u003d0"
            },
            {
              "jobUrl": "http://2d4da64570e5:4040/jobs/job?id\u003d1"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1661347456094_1084092688",
      "id": "paragraph_1661347456094_1084092688",
      "dateCreated": "2022-08-24 13:24:16.094",
      "dateStarted": "2022-08-24 16:59:41.020",
      "dateFinished": "2022-08-24 17:00:35.923",
      "status": "FINISHED"
    },
    {
      "text": "val sonnets \u003d sc.textFile(\"/usr/local/tpcds_data/call_center.dat\").flatMap(line \u003d\u003e line.split(\"|\")).map(word \u003d\u003e (word, 1)).reduceByKey(_ + _).count()\n",
      "user": "anonymous",
      "dateUpdated": "2022-08-24 16:38:19.792",
      "progress": 25,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34msonnets\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m \u003d 67\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://411c864c3440:4040/jobs/job?id\u003d5"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1661350086559_1101270536",
      "id": "paragraph_1661350086559_1101270536",
      "dateCreated": "2022-08-24 14:08:06.559",
      "dateStarted": "2022-08-24 16:38:19.802",
      "dateFinished": "2022-08-24 16:38:24.701",
      "status": "FINISHED"
    },
    {
      "text": "spark.read.csv(path)",
      "user": "anonymous",
      "dateUpdated": "2022-08-24 16:57:10.514",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:303)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:228)\norg.apache.zeppelin.spark.SparkScala212Interpreter.open(SparkScala212Interpreter.scala:88)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:122)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:844)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752)\norg.apache.zeppelin.scheduler.Job.run(Job.java:172)\norg.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\norg.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n\n  at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n  at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1526)\n  at org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106)\n  at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:132)\n  at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:123)\n  at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:232)\n  at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:457)\n  at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:448)\n  at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:535)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n  at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n  at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n  at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:238)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:340)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:473)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n  at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:112)\n  at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:65)\n  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n  at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\n  at scala.Option.orElse(Option.scala:447)\n  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n  at scala.Option.getOrElse(Option.scala:189)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:537)\n  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:443)\n  ... 44 elided\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1661350627801_1312154410",
      "id": "paragraph_1661350627801_1312154410",
      "dateCreated": "2022-08-24 14:17:07.808",
      "dateStarted": "2022-08-24 16:57:10.522",
      "dateFinished": "2022-08-24 16:57:10.917",
      "status": "ERROR"
    },
    {
      "text": "val ia: InetAddress \u003d InetAddress.getLocalHost\nval hostname: String \u003d ia.getHostName",
      "user": "anonymous",
      "dateUpdated": "2022-08-24 14:29:15.496",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mia\u001b[0m: \u001b[1m\u001b[32mjava.net.InetAddress\u001b[0m \u003d 26c61459aabe/10.28.5.2\n\u001b[1m\u001b[34mhostname\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d 26c61459aabe\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1661351093786_972663227",
      "id": "paragraph_1661351093786_972663227",
      "dateCreated": "2022-08-24 14:24:53.786",
      "dateStarted": "2022-08-24 14:29:15.508",
      "dateFinished": "2022-08-24 14:29:15.789",
      "status": "FINISHED"
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1661351168177_2045142415",
      "id": "paragraph_1661351168177_2045142415",
      "dateCreated": "2022-08-24 14:26:08.178",
      "status": "READY"
    }
  ],
  "name": "start 1",
  "id": "2HC5G6XRG",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}